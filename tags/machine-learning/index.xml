<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine-Learning on Clingm&#39;s Blog</title>
    <link>https://clingm.github.io/tags/machine-learning/</link>
    <description>Recent content in Machine-Learning on Clingm&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>© Clingm</copyright>
    <lastBuildDate>Mon, 28 Oct 2024 19:24:25 +0800</lastBuildDate>
    <atom:link href="https://clingm.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Machine Learning Start -- Kmeans</title>
      <link>https://clingm.github.io/posts/machine-learning/ml-start-kmeans/</link>
      <pubDate>Sat, 27 Jan 2024 17:03:21 +0800</pubDate>
      <guid>https://clingm.github.io/posts/machine-learning/ml-start-kmeans/</guid>
      <description>&lt;h2 id=&#34;instro&#34;&gt;instro&lt;/h2&gt;&#xA;&lt;p&gt;K均值聚类（K-Means clustering）是一种常见的无监督学习算法，用于将数据集划分为K个簇。&lt;/p&gt;&#xA;&lt;p&gt;算法伪代码如下&#xA;&lt;img src=&#34;https://s2.loli.net/2024/01/27/l7Q3C948J5UyznH.png&#34; alt=&#34;图片.png&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;使用西瓜数据集4.0来对算法进行测试&#xA;&lt;img src=&#34;https://s2.loli.net/2024/01/27/8gUIJmxhQ1MvOwc.png&#34; alt=&#34;图片.png&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;main-idea&#34;&gt;main idea&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;距离算法使用欧几里得距离，与sklearn和matlab一致&lt;/li&gt;&#xA;&lt;li&gt;使用最大迭代次数和质心偏移量来控制算法迭代过程&lt;/li&gt;&#xA;&lt;li&gt;使用簇内距离的平均值作为模型评估的标准&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;implement&#34;&gt;implement&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;kmeans&lt;/span&gt;(k, X, max_iter&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; isinstance(X, pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_numpy()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# init vec&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    centroids &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X[np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;choice(len(X), k, replace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;)]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(max_iter):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# calc distance&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        distances &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linalg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;norm(X[:, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;newaxis] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; centroids, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argmin(distances, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# new center&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        new_centroids &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([X[labels &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; i]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(k)])&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linalg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;norm(new_centroids &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; centroids) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1e-4&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        centroids &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; new_centroids&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; labels, centroids, distances&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;簇内平方和&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Start -- Decision Tree</title>
      <link>https://clingm.github.io/posts/machine-learning/ml-start-decision-tree/</link>
      <pubDate>Tue, 23 Jan 2024 18:53:18 +0800</pubDate>
      <guid>https://clingm.github.io/posts/machine-learning/ml-start-decision-tree/</guid>
      <description>&lt;h2 id=&#34;intro&#34;&gt;intro&lt;/h2&gt;&#xA;&lt;p&gt;西瓜书的决策树部分，在这篇文章中，将主要实现书中的&lt;code&gt;TreeGenerate&lt;/code&gt;算法以及各种选取&#xA;最优划分属性的方式。&lt;/p&gt;&#xA;&lt;p&gt;决策树在进行学习的时候有一个很要命的问题，就是过拟合。因为决策树会把所有的样本特征&#xA;都学习到，所以过拟合是很难避免的，虽然可以使用剪枝的方法来减少过拟合。不过这篇文章&#xA;并不会涉及到剪枝的内容。&lt;/p&gt;&#xA;&lt;p&gt;数据集依然使用西瓜书对应的西瓜数据集&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    色泽  根蒂  敲声  纹理  脐部  触感 好瓜&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;   青绿  蜷缩  浊响  清晰  凹陷  硬滑  是&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;   乌黑  蜷缩  沉闷  清晰  凹陷  硬滑  是&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;   乌黑  蜷缩  浊响  清晰  凹陷  硬滑  是&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;   青绿  蜷缩  沉闷  清晰  凹陷  硬滑  是&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;   浅白  蜷缩  浊响  清晰  凹陷  硬滑  是&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;   青绿  稍蜷  浊响  清晰  稍凹  软粘  是&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;   乌黑  稍蜷  浊响  稍糊  稍凹  软粘  是&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;   乌黑  稍蜷  浊响  清晰  稍凹  硬滑  是&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;   乌黑  稍蜷  沉闷  稍糊  稍凹  硬滑  否&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;   青绿  硬挺  清脆  清晰  平坦  软粘  否&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;  浅白  硬挺  清脆  模糊  平坦  硬滑  否&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt;  浅白  蜷缩  浊响  模糊  平坦  软粘  否&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;  青绿  稍蜷  浊响  稍糊  凹陷  硬滑  否&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;13&lt;/span&gt;  浅白  稍蜷  沉闷  稍糊  凹陷  硬滑  否&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;  乌黑  稍蜷  浊响  清晰  稍凹  软粘  否&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;  浅白  蜷缩  浊响  模糊  平坦  硬滑  否&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;  青绿  蜷缩  沉闷  稍糊  稍凹  硬滑  否&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;best-feature-to-split&#34;&gt;best feature to split&lt;/h2&gt;&#xA;&lt;p&gt;划分数据集的方式有基于信息增益，信息增益率和基尼指数。笔者只实现了信息增益和基尼指数&#xA;，因为信息增益率实现起来其实就和信息增益一样，只不过加了一个惩罚在里面。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Start -- Linear</title>
      <link>https://clingm.github.io/posts/machine-learning/ml-start-linear/</link>
      <pubDate>Sun, 21 Jan 2024 21:52:06 +0800</pubDate>
      <guid>https://clingm.github.io/posts/machine-learning/ml-start-linear/</guid>
      <description>&lt;h2 id=&#34;instro&#34;&gt;instro&lt;/h2&gt;&#xA;&lt;p&gt;线性回归是在特征与输出之间构建一个线性方程。如$ y = \sum_{i=0}^{n}a_{i}x_{i} + b$, $x, y$分别为特征向量和输出。&#xA;$a$作为需要求解的系数。一般的线性回归适用于回归任务（连续值），对于分类任务则使用对数几率回归，又称&#xA;逻辑斯特回归，使用对数几率函数$ f(x) = \frac{1}{1-e^{-x}} $。对于多分类任务则使用Softmax回归。&lt;/p&gt;&#xA;&lt;p&gt;线性回归问题，书本上给了两种求解方法：1. 最小二乘法 2. 梯度下降法。本篇文章将只实现最小二乘法，&#xA;因为梯度下降在神经网络中更加适合&lt;/p&gt;&#xA;&lt;h2 id=&#34;linear-regression&#34;&gt;Linear Regression&lt;/h2&gt;&#xA;&lt;h3 id=&#34;basic-model&#34;&gt;basic model&lt;/h3&gt;&#xA;&lt;p&gt;特征向量$x$，输出$y$，有&#xA;$$&#xA;\begin{aligned}&#xA;y &amp;amp;= a_0x_0 + a_1x_1 + \cdots + a_{i-1}x_{n-1} + b \\&#xA;&amp;amp;= \sum_{i=0}^{n-1}a_ix_i + b&#xA;\end{aligned}&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;把$a$和$b$吸收入向量形式$ w=(a;b) $, 相应的，把数据集$D$表示为一个$m×(d+1)$大小的矩阵，其中每行对应于一个示例，该行前d个元素对应于示例的d个属性值，最后一个元素恒置为1，即：&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\mathbf{X}=\begin{pmatrix}x_{11}&amp;amp;x_{12}&amp;amp;\ldots&amp;amp;x_{1d}&amp;amp;1\\&#xA;x_{21}&amp;amp;x_{22}&amp;amp;\ldots&amp;amp;x_{2d}&amp;amp;1\\&#xA;\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots&amp;amp;\vdots\\&#xA;x_{m1}&amp;amp;x_{m2}&amp;amp;\ldots&amp;amp;x_{md}&amp;amp;1\end{pmatrix}=\begin{pmatrix}\boldsymbol{x}_1^\mathrm{T}&amp;amp;1\\x_2^\mathrm{T}&amp;amp;1\\ \vdots&amp;amp;\vdots\\x_m^\mathrm{T}&amp;amp;1\end{pmatrix}&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;Y = Xw&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;损失函数选择均方差损失&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\begin{aligned}&#xA;L &amp;amp;= \sum_{i=0}^{n-1}(y - \hat{y})^2 \\&#xA;&amp;amp;= \sum_{i=0}^{n-1}(y - \sum_{i=0}^{n-1}a_ix_i - b)^2 \\&#xA;&amp;amp;= (Y - Xw)^2&#xA;\end{aligned}&#xA;$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Start</title>
      <link>https://clingm.github.io/posts/machine-learning/ml-start/</link>
      <pubDate>Sun, 21 Jan 2024 20:02:14 +0800</pubDate>
      <guid>https://clingm.github.io/posts/machine-learning/ml-start/</guid>
      <description>&lt;p&gt;以此篇作为开头，开一个新坑是关于西瓜书机器学习各种算法的实验。&lt;/p&gt;&#xA;&lt;p&gt;尽请期待吧。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
